# -*- coding: utf-8 -*-
"""ML-Lab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aL9zsd5H4k1r4fP0mF6gRF5rQX0lOZJa

# Assignment 1
"""

!pip install mat4py

import pandas as pd
import scipy.io as sio
from sklearn.model_selection import train_test_split
import os
import mat4py as mt
import numpy as np
import matplotlib.pyplot as plt
import sys
from sklearn.metrics import mean_squared_error, mean_absolute_error
from google.colab import drive
drive.mount('/content/gdrive')
root_path = 'gdrive/My Drive/ML/'

"""### OpenFile function defined to open the  files and convert the data to the pandas dataframe
New training and testing data will be saved "new_train.ext" and "new_test.ext" files respectively (ext = extension)

BT19ECE036_dataset_div_shuffle(filepath, ratio, strY, *strX):
### filepath, train ratio, dependent variables, independent variables
"""

def openFile(filepath):
  data = None
  with open(filepath, 'r') as fp:
    extension = filepath.split(".")[-1]
  if (extension == 'xlsx'):
    data = pd.read_excel(filepath)
  elif (extension == 'csv'):
    data = pd.read_csv(filepath)
  elif (extension == 'mat'):
    datamat = mt.loadmat(filepath)
    data = pd.DataFrame.from_dict(datamat)
  else:
    print("File not found!")
    return None
  return [data, extension]

def BT19ECE036_dataset_div_shuffle(filepath, ratio, strY, strX):
  data, extension = openFile(filepath)
  data.columns = data.columns.str.strip()
  dataX = data.filter(strX)
  dataY = data.filter(strY)
  x_train, x_test, y_train, y_test = train_test_split(dataX, dataY, train_size = ratio, shuffle=True)
  saveFile(x_train, x_test, y_train, y_test, extension)
  return data

def saveFile(x_train, x_test, y_train, y_test, extension):
  if (extension == 'csv'):
    x_train.to_csv("x_train.csv")
    x_test.to_csv("x_test.csv")
    y_train.to_csv("y_train.csv")
    y_test.to_csv("y_test.csv")
  elif (extension == 'xlsx'):
    x_train.to_excel("x_train.xlsx")
    x_test.to_excel("x_test.xlsx")
    y_train.to_excel("y_train.xlsx")
    y_test.to_excel("y_test.xlsx")
  elif (extension == 'mat'):
    sio.savemat("x_train.mat", x_train)
    sio.savemat("x_test.mat", x_test)
    sio.savemat("y_train.mat", y_train)
    sio.savemat("y_test.mat", y_test)
  else:
    print("Unable to save file")

"""### Dividing original dataset into training and testing datasets with a split ration of 0.75"""

filepath = root_path + 'train.csv'
print(filepath)
dataLoad = pd.read_csv(filepath)
data = BT19ECE036_dataset_div_shuffle(filepath, 0.75, dataLoad.columns[3], dataLoad.columns[2])
data

"""# Assignment 2"""

filepath = root_path + 'Matlab_accidents.mat'
dataLoad = mt.loadmat(filepath)
datamat = dataLoad['accidents']
data = pd.DataFrame(datamat['hwydata'], columns = datamat['hwyheaders'])
states = [x[0] for x in datamat["statelabel"]]
data.insert(loc = 0,column = 'States',value = states)
filepath = "new_accidents.csv"
data.to_csv(filepath)
data

"""### Using function defined in experiment 1 to perform train test split for new matlab data file. """

independent = ['Licensed drivers (thousands)', 'Registered vehicles (thousands)', 'Vehicle-miles traveled (millions)']
dependent = ['Miles traveled per vehicle']
data = BT19ECE036_dataset_div_shuffle(filepath, 0.7, dependent, independent)
data

"""### Normalising train and test data and removing unamed column which was a result of storing and retrieving data from new files, index got converted into unnamed column"""

def newData(filepath):
  data, extension = openFile(filepath)

  for column in data:
    data[column] = data[column]/np.amax(data[column])
  return data
dataXTrain = newData('x_train.csv')
dataYTrain = newData('y_train.csv')
dataYTest = newData('y_test.csv')
dataXTest = newData('x_test.csv')
dataXTrain.drop(dataXTrain.columns[0], axis = 1, inplace = True)
dataYTrain.drop(dataYTrain.columns[0], axis = 1, inplace = True)
dataXTest.drop(dataXTest.columns[0], axis = 1, inplace = True)
dataYTest.drop(dataYTest.columns[0], axis = 1, inplace = True)

dataYTrain

"""### Training the model on training dataset and testing on X testing dataset. Then predicting on Y test dataset"""

def theta(dataXTrain, dataYTrain, dataYTest, dataXTest):
  dataXtrain = np.array(dataXTrain)
  dataYtrain = np.array(dataYTrain)
  dataYTest = np.array(dataYTest)
  dataXTest = np.array(dataXTest)

  trainOnes = np.ones([dataXTrain.shape[0], 1])
  thetaTrain = np.hstack((trainOnes, dataXTrain))

  testOnes = np.ones([dataXTest.shape[0], 1])
  thetaTest = np.hstack((testOnes, dataXTest))

  return thetaTrain, thetaTest


def linregPseudoInverse(dataXTrain, dataYTrain, dataYTest, dataXTest):
  thetaTrain, thetaTest = theta(dataXTrain, dataYTrain, dataYTest, dataXTest)
  weightOpt = np.matmul(np.linalg.pinv(thetaTrain), dataYTrain)
  yPredict = np.matmul(thetaTest, weightOpt)
  return [dataYTest, yPredict]

"""### Error Calculation"""

def errors(x, y):
  print("Mean Squared Error: ", mean_squared_error(x, y))
  print("Root Mean Squared Error: ", mean_squared_error(x, y, squared = False))
  print("Mean Absolute Error: ", mean_absolute_error(x, y))

"""### Plotting the real value and predicted graphs recieved after training the model over the datasets."""

x, y = linregPseudoInverse(dataXTrain, dataYTrain, dataYTest, dataXTest)
plt.plot(x)
plt.plot(y)
errors(x, y)

"""### Predicted in orange and Y-test in blue

## Finding optimum weights using Gradiend Descent
"""

def gradientDescent(iterations, learningRate, dataXTrain, dataYTrain, dataYTest, dataXTest):
  thetaTrain, thetaTest = theta(dataXTrain, dataYTrain, dataYTest, dataXTest)
  gradWieght = np.random.randn(len(thetaTrain[0]), 1)
  
  for i in range(iterations):
    deltaE = np.matmul(thetaTrain.transpose(), (np.matmul(thetaTrain, gradWieght) - dataYTrain))
    gradWieght = gradWieght - learningRate * deltaE 
  yGradientPrediction = np.matmul(thetaTest, gradWieght)
  return dataYTest, yGradientPrediction

"""### Plotting Y-test and predicted values and calculating the error ratio"""

x, y = gradientDescent(1000, 0.02, dataXTrain, dataYTrain, dataYTest, dataXTest)
plt.plot(x)
plt.plot(y)
errors(x, y)

"""### Predicted in orange and Y-test in blue

## Changing relationship between input and output variables

### Recalculating Theta matrix
"""

trainOnes = np.ones([dataXTrain.shape[0], 1])

trainSquare = np.square(dataXTrain.T.iloc[0]).values.reshape(dataXTrain.shape[0], 1)
thetaTrain = np.hstack((trainOnes, trainSquare, dataXTrain))
# thetaTrain = np.hstack((trainOnes, np.reshape(np.square(dataXTrain.T.iloc[0]).T, (dataXTrain.shape[0], 1)), dataXTrain))
# thetaTest = np.hstack((testOnes, np.reshape(np.square(dataXTest.T[0]).T, (dataXTest.shape[0], 1)), dataXTest))
testOnes = np.ones([dataXTest.shape[0], 1])
trainSquare = np.square(dataXTest.T.iloc[0]).values.reshape(dataXTest.shape[0], 1)
thetaTest = np.hstack((testOnes, trainSquare, dataXTest))

"""### Calculate Gradient Wieght and using Pseudo Inverse"""

gradWieght = np.matmul(np.linalg.pinv(thetaTrain), dataYTrain)
yPrediction = np.matmul(thetaTest, gradWieght)

x = dataYTest
y = yPrediction
plt.plot(x)
plt.plot(y)
errors(x, y)

"""# Assignment 3"""

data = mt.loadmat(root_path + "Matlab_cancer.mat")

import math
dataX = data['x']
dataY = data['t']
tr = np.transpose(np.vstack([dataY, dataX]))
np.random.shuffle(tr)
trnspose = np.transpose(tr)
dataY = trnspose[0, : ]
dataX = trnspose[2 : , : ]
splitRatio = 0.7
p = math.ceil(dataX.shape[1] * splitRatio)
print("training dataset:-", p)
dataXTrain = dataX[:, : p // 2]
dataYTrain = dataY[ : p // 2]
dataXTest = dataX[ : , p : ]
dataYTest = dataY[p:]
val_x = dataX[ : , p // 2 : p]
val_y = dataY[p // 2 : p]
np.count_nonzero(dataXTrain)

class NeuralNetwork(): 
    def __init__(self, model, learningRate): 
        self.architecture = model
        self.neuralLayers = len(model)
        self.learningRate = learningRate
        self.dw = []
        self.db = []
        self.bias = []
        self.weight = []
        self.cost = []
        self.accuracyTest = []
        self.accuracyTrain = []
        self.accuracyValidation = []
        
        for i, j in zip(model[1 : ], model[ : -1]): 
            w = np.random.randn(i, j) 
            b = np.random.randn(i, 1) 
            dw = np.zeros([i, j])
            db = np.zeros([i, 1])
            self.dw.append(dw)
            self.db.append(db)
            self.weight.append(w)
            self.bias.append(b)
        self.activation = []
        for i in model: 
            a = np.zeros(i)
            self.activation.append(a)

    def sigmoid(self, z): 
        activation = 1 / (1 + np.exp(-z))
        return activation

    def costFunction(self, Y): 
        L = (Y * np.log(self.activation[-1]) + (1 - Y) * np.log(1 - self.activation[-1]))
        L = -L
        J = np.sum(L) / Y.shape[0]
        self.cost.append(J)

    def forwardPropagation(self, ip): 
        activation = ip
        self.activation[0] = activation
        saveNum = list(range(1, self.neuralLayers))
        for i, w, b in zip(saveNum, self.weight, self.bias): 
            z = np.matmul(w, activation) + b
            activation = self.sigmoid(z) 
            self.activation[i] = activation 
    
    def backwardPropagation(self, batchSize, Y): 
        dz = self.activation[-1] - Y
        dw = np.matmul(dz, self.activation[-2].T) / batchSize
        db = np.sum(dz, axis = 1) / batchSize
        self.dw[-1] = dw
        self.db[-1] = db.reshape([-1, 1])
        for i in range(2, self.neuralLayers):        
            sis = self.activation[-i] * (1 - self.activation[-i])
            dz = np.matmul(self.weight[-i + 1].T, dz) 
            dz = dz * sis      
            dw = np.matmul(dz, self.activation[-i - 1].T) / batchSize
            db = np.sum(dz, axis=1) / batchSize
            self.dw[-i] = dw
            self.db[-i] = db.reshape([-1, 1])
            self.costFunction(Y)

    def gradientDescent(self): 
        for i in range(self.neuralLayers - 1): 
            self.weight[i] = self.weight[i] - self.learningRate * self.dw[i]
            self.bias[i] = self.bias[i] - self.learningRate * self.db[i]

    def accuracy(self, ip, op, threshold = 0.5, confusion = False): 
        activation = ip
        saveN = list(range(1, self.neuralLayers))
        accuracy = 0
        for i, w, b in zip(saveN, self.weight, self.bias):
          z = np.matmul(w, activation) + b   
          activation = self.sigmoid(z)  
        activation = activation.reshape(-1, )
        activation[activation > threshold] = 1
        activation[activation <= threshold] = 0
        if confusion == True: 
            return activation
        for i, j in enumerate(activation): 
            if j == op[i]: 
                accuracy = accuracy+1
        return accuracy / ip.shape[1]

    def confusionMatrix(self, ip, op, thresholdList): 
        T_p = []
        T_n = []
        F_p = []
        F_n = []
        for i in thresholdList: 
            activation = self.accuracy(ip, op, i, True)
            c = activation - 2 * op
            T_p.append(np.count_nonzero(c == -1))
            T_n.append(np.count_nonzero(c == 0))
            F_p.append(np.count_nonzero(c == 1))
            F_n.append(np.count_nonzero(c == -2))
        return T_p, T_n, F_p, F_n

    def trainFunction(self, ip, op, epochs,  validationSet = None): 
        batchSize = ip.shape[1]
        for i in range(epochs): 
            self.forwardPropagation(ip)
            self.backwardPropagation(batchSize, op)
            self.gradientDescent()
            accTrain = self.accuracy(ip, op)
            print('Accuracy = ',  accTrain * 100, '%')
            self.accuracyTrain.append( accTrain)
            if validationSet != None: 
                accTest = self.accuracy( validationSet[0],  validationSet[1])
                self.accuracyTest.append( accTest )

annObject = NeuralNetwork([100, 32, 1], 3e-2)

thresholdList = np.arange(0, 1, 0.001)
T_p, T_n, F_p, F_n = annObject.confusionMatrix(dataXTrain, dataYTrain, thresholdList)
T_p = np.array(T_p)
T_n = np.array(T_n)
F_p = np.array(F_p)
F_n = np.array(F_n)

"""### Without validation"""

annObject.neuralLayers
annObject.trainFunction(dataXTrain, dataYTrain, 600)

"""# ###With validation"""

annObject.trainFunction(dataXTrain, dataYTrain, 600, [val_x, val_y])

"""###*Specificity - speci*
###*Sensitivity - sensi*
"""

speci = T_n/(F_p+T_n)
sensi = T_p/(T_p+F_n)
f_r = 1 - speci

print(speci)

print(sensi)

from mlxtend.plotting import plot_confusion_matrix
thresholdA = np.arange(0, 1, 0.1)
Tp,Tn,Fp,Fn = annObject.confusionMatrix(dataXTrain, dataYTrain, thresholdA)
Tp = np.array(Tp)
Tn = np.array(Tn)
Fp = np.array(Fp)
Fn = np.array(Fn)
for i, j, k, l, thresholdValue in zip(Tp, Tn, Fp, Fn, thresholdList):
    print(i, j, k, l)
    confusionMatrix = np.array([[j, k], [l, i]])
    fig, ax = plot_confusion_matrix(conf_mat = confusionMatrix, figsize = (3, 3), cmap = plt.cm.Blues)
    plt.xlabel('Predicted Value')
    plt.ylabel('True Value')
    plt.title('ConfusionMatrix, threshold = {}'.format(round(thresholdValue, 3)), fontsize = 15)
    plt.show()

plt.plot(annObject.accuracyTrain)

annObject.accuracy(dataXTest, dataYTest)